{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nueural Networks\n",
    "- ANN mimics biological neurons\n",
    "- In a braincell dendrites passes the electrical signals to the body of the cell and then later on single electrical \n",
    "signal(output)passes on Axon to connect to some other neuron.\n",
    "\n",
    "Perceptron Model(ANN Neurons)\n",
    "<img src=\"perceptron.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Mathematical representation for A perceptron is;\n",
    "<img src=\"math-representation.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "- It is also possible to connect many perceptrons together and represent them mathematically.\n",
    "- Here is an example;\n",
    "\n",
    "<img src=\"basic-nn.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "- If you have more than 3 layers that's considered as deep network\n",
    "- Also note that as you go through many layers the level of abstraction increases\n",
    "\n",
    "## Activation Functions\n",
    " **Sigmoid Function**\n",
    " \n",
    " z = w.x+b\n",
    " \n",
    " It can also be represented as f(z) = 1 / 1 + e^(-z)\n",
    " \n",
    " Output can be between 0-1\n",
    " \n",
    " \n",
    " **Hyperbolic Tangent: tanh(z)**\n",
    " \n",
    " sinh(z) = (e^x - e^-x)/2\n",
    " \n",
    " cosh(z) = (e^x + e^-x)/2\n",
    " \n",
    " Mathematically it is tanh(z) = sinh(z)/cosh(z)\n",
    " \n",
    " The output can be between -1-1\n",
    "\n",
    "\n",
    " **Rectified Linear Unit (ReLU)**\n",
    " \n",
    " Relatively simple\n",
    "\n",
    " Function is max(0,z) for example if z = -number, then the function will return 0. If z is positive then it will return the max value of z\n",
    "\n",
    " Currently kind of considered as state of the art\n",
    "\n",
    " Notes: Usually Relu and tanh gives the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions\n",
    "\n",
    "1. We use cost function to evaluate performance of a neuron.\n",
    "2. We can use a cost function to measure how far off we are from the expected value\n",
    "\n",
    "Notation: y => True value, a => Neuron's prediction, \n",
    "\n",
    "In terms of weight and bias: \n",
    "- w*x+b = z (weight*input)+bias\n",
    "- Pass z into an activation function for example sigma(z) = a (for sigmoid function)\n",
    "### Quadratic Function\n",
    "\n",
    " C = E(y-a)^2 / n\n",
    "- We can see that larger errors are more prominent due to the squaring\n",
    "- This calculation can cause slowdown in our learning speed because of the way it works\n",
    "\n",
    "### Cross Entropy\n",
    "C = (-1/n) E(y.ln(a) + (1-y) * ln(1-a)\n",
    "- Using this const function allows faster learning\n",
    "- The larger the difference, the faster the neuron can learn\n",
    "\n",
    "## Gradient Descent and Back Propogation\n",
    "\n",
    "- Gradient descent is an optimization algorithm for finding the minimum of a function\n",
    "- To find a local minimum, we take steps proportional to the negative of the gradient\n",
    "- Using Gradient Descent we can figure out the best parameters for minimizing our cost. For example,a finding the best values for the weights of the neuron inputs. \n",
    "- Backpropagation is used to calculate the error contribution of each neuron after a batch of data is processed. \n",
    "- It relies heavily on the chain rule to go back through the network and calculate these errors. \n",
    "- Back Propogation works by calculating error at the output and then distributes back through the network layer.\n",
    "- It requires a known desired output for each input value(Supervised learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
